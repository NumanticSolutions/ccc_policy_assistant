{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "Crawl a bunch of sites and save data to BQ"
   ],
   "id": "f1fedc7276f4ce57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T00:52:29.522837Z",
     "start_time": "2024-12-12T00:52:29.517246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Asynchronous work\n",
    "import asyncio\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "\n",
    "# Web crawling and scraping tools class\n",
    "sys.path.insert(0, \"../../scrapers/\")\n",
    "import web_scraper as ws\n",
    "import web_crawler as wc\n"
   ],
   "id": "4f4a2d7c6ed44de7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the site scraper on one site",
   "id": "531fd4b16f90481e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:17:35.365488Z",
     "start_time": "2024-12-11T22:16:56.428934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "turl = \"https://en.wikipedia.org/wiki/Chaffey_College\"\n",
    "turl = \"https://asdjhfbads.jhbf\"\n",
    "turl = \"https://www.cccco.edu/\"\n",
    "\n",
    "test = await ws.webScraper.visit_page(url=turl)\n",
    "\n",
    "test.crawl_results.keys()\n",
    "# type(test.crawl_results)\n"
   ],
   "id": "102ebea55bd6337c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Chromium download.\n",
      "100%|██████████| 141M/141M [00:05<00:00, 25.3Mb/s] \n",
      "[INFO] Beginning extraction\n",
      "[INFO] Chromium extracted to: /Users/stephengodfrey/Library/Application Support/pyppeteer/local-chromium/1181205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'html_code_string', 'soup', 'ptag_text', 'atag_urls'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web Crawler",
   "id": "fbe52c87cf88b4ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up GCP credentials",
   "id": "e3efb12e06e36a81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set GCP credentials\n",
    "creds_path = \"../../../data/environment/\"\n",
    "creds_file = \"eternal-bongo-435614-b9-bf6a5e630e44.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(filename=os.path.join(creds_path,\n",
    "                                                                                          creds_file))\n",
    "\n",
    "# Set credentials\n",
    "pandas_gbq.context.credentials = credentials\n"
   ],
   "id": "9f2116647c8605b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get a list of previously crawled websites",
   "id": "2b4cb5cc7b1be393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Query the BigQuery crawl results table to get a list of sites already crawled\n",
    "# Identify the table; note the database.table naming schema\n",
    "table_id = \"ccc_polasst.crawl_data\"\n",
    "project_id = \"eternal-bongo-435614-b9\"\n",
    "\n",
    "# Create a sql statement\n",
    "sql = (\"SELECT DISTINCT url FROM `{}` \").format(table_id)\n",
    "sql = (\"SELECT * FROM `{}` \").format(table_id)\n",
    "\n",
    "# Use Pandas-gbq to read from the BQ table\n",
    "# df_pc = pandas_gbq.read_gbq(sql, project_id=project_id)\n",
    "\n",
    "########### Get a list of previously crawled sites\n",
    "########### This needs some work - don't crawl may not work since we still need the URLs\n",
    "# crawled_urls = df_pc[\"url\"].unique().tolist()\n",
    "\n",
    "# dont_crawl_urls = []\n",
    "\n",
    "# len(crawled_urls)"
   ],
   "id": "b1cfc83f8c319e9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Crawl websites (and write results to OneDrive)",
   "id": "628ecc31e9da8437"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:29:54.820700Z",
     "start_time": "2024-12-12T00:53:05.224390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crawl seed_url and web links found on it and its child pages\n",
    "# seed_url = \"https://www.cccco.edu\"\n",
    "seed_url = \"https://en.wikipedia.org/wiki/California_Community_Colleges\"\n",
    "dont_crawl_urls = []\n",
    "\n",
    "crawler = wc.webCrawler(seed_url=seed_url)\n",
    "crlres = await crawler.crawl_sites(dont_crawl_urls=dont_crawl_urls,\n",
    "                                   depth=10,\n",
    "                                   width=300)\n"
   ],
   "id": "6827621e8a2dfdf3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:37,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 1: 1 URLs crawled; 552 URLs in to_crawl_urls; 2 URLs in dont_crawl_urls\n",
      "Batch 1 saved to disk\n",
      "Batch 2 saved to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [11:04<52:02, 390.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 2: 301 URLs crawled; 39579 URLs in to_crawl_urls; 602 URLs in dont_crawl_urls\n",
      "Batch 3 saved to disk\n",
      "Batch 4 saved to disk\n",
      "Batch 5 saved to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [22:03<59:49, 512.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 3: 601 URLs crawled; 96971 URLs in to_crawl_urls; 1202 URLs in dont_crawl_urls\n",
      "Batch 6 saved to disk\n",
      "Batch 7 saved to disk\n",
      "Batch 8 saved to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [32:56<56:49, 568.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 4: 901 URLs crawled; 93154 URLs in to_crawl_urls; 1802 URLs in dont_crawl_urls\n",
      "Batch 9 saved to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [36:38<36:56, 443.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 5: 1000 URLs crawled; 43474 URLs in to_crawl_urls; 2402 URLs in dont_crawl_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [36:40<19:32, 293.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 6: 1001 URLs crawled; 161 URLs in to_crawl_urls; 3002 URLs in dont_crawl_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [36:42<09:54, 198.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 7: 1002 URLs crawled; 193 URLs in to_crawl_urls; 3324 URLs in dont_crawl_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [36:44<04:31, 135.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 8: 1003 URLs crawled; 500 URLs in to_crawl_urls; 3710 URLs in dont_crawl_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [36:47<01:34, 94.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 saved to disk\n",
      "Depth level finished: 9: 1004 URLs crawled; 952 URLs in to_crawl_urls; 4310 URLs in dont_crawl_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [36:49<00:00, 220.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth level finished: 10: 1005 URLs crawled; 0 URLs in to_crawl_urls; 4910 URLs in dont_crawl_urls\n",
      "Batch 11 saved to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dir(crlres)\n",
    "# print(\"This crawl dataframe is {} rows.\".format(len(df)))\n"
   ],
   "id": "f5252ca4f89f457e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read local crawl results",
   "id": "855382389ec522c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:35:59.417755Z",
     "start_time": "2024-12-12T01:35:59.073677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "crawl_data_path  = (\"/Users/stephengodfrey/OneDrive - numanticsolutions.com\"\n",
    "                    \"/Engagements/Projects/ccc_policy_assistant/data/crawls\")\n",
    "crawl_file = \"enwikipediaorg_2024Dec11_10.csv\"\n",
    "\n",
    "os.listdir(crawl_data_path)\n",
    "\n",
    "dft = pd.read_csv(filepath_or_buffer=os.path.join(crawl_data_path,crawl_file))\n"
   ],
   "id": "2bf115fb87b57ca9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save results to BigQuery",
   "id": "83361bdc988aaf79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Identify the table; note the database.table naming schema\n",
    "table_id = \"ccc_polasst.crawl_data\"\n",
    "table_id = \"ccc_polasst.crawl_data_tes\"\n",
    "project_id = \"eternal-bongo-435614-b9\"\n",
    "\n",
    "# Use Pandas-gbq to write to the BQ table\n",
    "if_exists = \"replace\"\n",
    "# if_exists = \"append\"\n",
    "\n",
    "# pandas_gbq.to_gbq(dataframe=df, destination_table=table_id,\n",
    "#                   project_id=project_id, if_exists=if_exists)\n",
    "\n"
   ],
   "id": "b5fe78acd3c381d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7fe67adac08eda20"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
