{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "Crawl a bunch of sites and save data to BQ"
   ],
   "id": "f1fedc7276f4ce57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Asynchronous work\n",
    "import asyncio\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "\n",
    "# Web crawling and scraping tools class\n",
    "sys.path.insert(0, \"../../scrapers/\")\n",
    "import web_scraper as ws\n",
    "import web_crawler as wc\n"
   ],
   "id": "4f4a2d7c6ed44de7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the site scraper on one site",
   "id": "531fd4b16f90481e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "turl = \"https://en.wikipedia.org/wiki/Chaffey_College\"\n",
    "turl = \"https://asdjhfbads,jhbf\"\n",
    "turl = \"https://www.cccco.edu/\"\n",
    "\n",
    "test = await ws.webScraper.visit_page(url=turl)\n",
    "\n",
    "test.crawl_results.keys()\n",
    "# type(test.crawl_results)\n"
   ],
   "id": "102ebea55bd6337c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web Crawler",
   "id": "fbe52c87cf88b4ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up GCP credentials",
   "id": "e3efb12e06e36a81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set GCP credentials\n",
    "creds_path = \"../../../data/environment/\"\n",
    "creds_file = \"eternal-bongo-435614-b9-bf6a5e630e44.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(filename=os.path.join(creds_path,\n",
    "                                                                                          creds_file))\n",
    "\n",
    "# Set credentials\n",
    "pandas_gbq.context.credentials = credentials\n"
   ],
   "id": "9f2116647c8605b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get a list of previously crawled websites",
   "id": "2b4cb5cc7b1be393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Query the BigQuery crawl results table to get a list of sites already crawled\n",
    "# Identify the table; note the database.table naming schema\n",
    "table_id = \"ccc_polasst.crawl_data\"\n",
    "project_id = \"eternal-bongo-435614-b9\"\n",
    "\n",
    "# Create a sql statement\n",
    "sql = (\"SELECT DISTINCT url FROM `{}` \").format(table_id)\n",
    "sql = (\"SELECT * FROM `{}` \").format(table_id)\n",
    "\n",
    "# Use Pandas-gbq to read from the BQ table\n",
    "# df_pc = pandas_gbq.read_gbq(sql, project_id=project_id)\n",
    "\n",
    "########### Get a list of previously crawled sites\n",
    "########### This needs some work - don't crawl may not work since we still need the URLs\n",
    "# crawled_urls = df_pc[\"url\"].unique().tolist()\n",
    "\n",
    "# dont_crawl_urls = []\n",
    "\n",
    "# len(crawled_urls)"
   ],
   "id": "b1cfc83f8c319e9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Crawl websites",
   "id": "628ecc31e9da8437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Crawl seed_url and web links found on it and its child pages\n",
    "seed_url = \"https://www.cccco.edu\"\n",
    "# seed_url = \"https://en.wikipedia.org/wiki/California_Community_Colleges\"\n",
    "dont_crawl_urls = []\n",
    "\n",
    "crawler = wc.webCrawler(seed_url=seed_url)\n",
    "df = await crawler.crawl_sites(dont_crawl_urls=dont_crawl_urls,\n",
    "                               depth=1,\n",
    "                               width=2)\n"
   ],
   "id": "6827621e8a2dfdf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df\n",
    "print(\"The crawl dataframe is {} rows.\".format(len(df)))\n"
   ],
   "id": "f5252ca4f89f457e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read local crawl results",
   "id": "855382389ec522c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "crawl_data_path  = (\"/Users/stephengodfrey/OneDrive - numanticsolutions.com\"\n",
    "                    \"/Engagements/Projects/ccc_policy_assistant/data/crawls\")\n",
    "crawl_file = \"wwwccccoedu_2024Dec11_1.csv\"\n",
    "\n",
    "os.listdir(crawl_data_path)\n"
   ],
   "id": "2bf115fb87b57ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read data file to test save\n",
    "dft2 = pd.read_csv(filepath_or_buffer=os.path.join(crawl_data_path, crawl_file))\n",
    "dft2"
   ],
   "id": "c70c10d2c60bbc69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save results to BigQuery",
   "id": "83361bdc988aaf79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Identify the table; note the database.table naming schema\n",
    "table_id = \"ccc_polasst.crawl_data\"\n",
    "table_id = \"ccc_polasst.crawl_data_tes\"\n",
    "project_id = \"eternal-bongo-435614-b9\"\n",
    "\n",
    "# Use Pandas-gbq to write to the BQ table\n",
    "if_exists = \"replace\"\n",
    "# if_exists = \"append\"\n",
    "\n",
    "# pandas_gbq.to_gbq(dataframe=df, destination_table=table_id,\n",
    "#                   project_id=project_id, if_exists=if_exists)\n",
    "\n"
   ],
   "id": "b5fe78acd3c381d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7fe67adac08eda20"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
